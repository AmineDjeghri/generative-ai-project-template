{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Generative AI Project Template <p>Template for a new AI Cloud project.</p> <p>Click on Use this template to start your own project!</p> <p></p> <p>This project is a generative ai template. It contains the following features: LLMs, information extraction, chat, rag &amp; evaluation. It uses LLMs(local or cloud),streamlit (with and without fastapi) &amp; Promptfoo as an evaluation and redteam framework for your AI system.</p>"},{"location":"#table-with-three-columns-for-three-photos","title":"table with three columns for three photos","text":"Evaluation with promptfoo Streamlit <p>Engineering tools:</p> <ul> <li>[x] Use UV to manage packages</li> <li>[x] pre-commit hooks: use <code>ruff</code> to ensure the code quality &amp; <code>detect-secrets</code> to scan the secrets in the code.</li> <li>[x] Logging using loguru (with colors)</li> <li>[x] Pytest for unit tests</li> <li>[x] Dockerized project (Dockerfile &amp; docker-compose): for the evaluation pipeline.</li> <li>[x] Streamlit (frontend) &amp; FastAPI (backend)</li> <li>[x] Make commands to handle everything for you: install, run, test</li> </ul> <p>AI tools:</p> <ul> <li>[x] LLM running locally (Ollama &amp; Ollamazure) or in the cloud (OpenAI &amp; Azure OpenAI)</li> <li>[x] Information extraction and Question answering from documents</li> <li>[x] Chat to test the AI system</li> <li>[x] Efficient async code using asyncio.</li> <li>[x] AI Evaluation framework: using Promptfoo, Ragas &amp; more...</li> </ul> <p>CI/CD &amp; Maintenance tools:</p> <ul> <li>[x] CI/CD pipelines: <code>.github/workflows</code> for GitHub and <code>.gitlab-ci.yml</code> for GitLab</li> <li>[x] Local CI/CD pipelines: GitHub Actions using <code>github act</code> and local GitLab CI using <code>gitlab-ci-local</code></li> </ul> <p>Documentation tools:</p> <ul> <li>[x] Wiki creation and setup of documentation website using Mkdocs</li> <li>[x] GitHub Pages deployment using mkdocs gh-deploy plugin</li> </ul> <p>Upcoming features:</p> <ul> <li>[ ] optimize caching in CI/CD</li> <li> (https://docs.github.com/en/communities/using-templates-to-encourage-useful-issues-and-pull-requests/creating-a-pull-request-template-for-your-repository)</li> <li>[ ] Additional MLOps templates: https://github.com/fmind/mlops-python-package</li> <li>[ ] Add MLFlow</li> <li>[ ] add Langfuse</li> <li>[ ] deploy gh pages in actions</li> </ul>"},{"location":"#1-getting-started","title":"1. Getting started","text":"<p>This project contains two parts:</p> <ul> <li>The AI app: contains an AI system (local or cloud), a frontend (streamlit), with an optional backend(fastapi).</li> <li>(optional)The Evaluation Tool: The evaluation tool is used to evaluate the performance and safety of the AI system. It uses promptfoo &amp; RAGAS, Python 3.11 and NVM are needed, but no need to install them by yourself since the project will handle that for you.</li> </ul> <p>The following files are used in the contribution pipeline:</p> <ul> <li><code>.env.example</code>: example of the .env file.</li> <li><code>.env</code> : contains the environment variables used by the app.</li> <li><code>Makefile</code>: contains the commands to run the app locally.</li> <li><code>Dockerfile</code>: the dockerfile used to build the docker image. It uses the Makefile commands to run the app.</li> <li><code>.pre-commit-config.yaml</code>: pre-commit hooks configuration file</li> <li><code>pyproject.toml</code>: contains the pytest, ruff &amp; other configurations.</li> <li><code>log_config.py</code>: logging configuration file for the project. This logger is used in the backend and can be used in   the frontend.</li> <li>.github/workflows/**.yml: GitHub actions configuration files.</li> <li>.gitlab-ci.yml: Gitlab CI configuration files.</li> </ul>"},{"location":"#11-local-prerequisites","title":"1.1. Local Prerequisites","text":"<ul> <li>Ubuntu 22.04 or MacOS</li> <li>git clone the repository</li> <li>UV &amp; Python 3.11 (will be installed by the Makefile)</li> <li>Create a <code>.env</code> file (take a look at the <code>.env.example</code> file)</li> </ul>"},{"location":"#steps-for-installation-users","title":"\u2699\ufe0f Steps for Installation (Users)","text":""},{"location":"#app-ai-fastapi-streamlit","title":"App (AI, FastAPI, Streamlit)","text":"<ol> <li>To install the app, run <code>make install-prod</code>.</li> <li>Choose one of the following options:</li> <li>Local model: we use Ollama that simulates OpenAI or Azure OpenAI. The model that is used is <code>phi3:3.8b-mini-4k-instruct-q4_K_M</code> but can be changed.<ul> <li>Read about how the app handles different providers and how to emulate OpenAI if you use open source models.</li> <li>Update the <code>.env</code> file (take a look at the <code>.env.example</code> file)</li> <li>Install Ollama (for openai) <code>make install-ollama</code> or ollamazure (for azure)</li> <li>Download the model, run <code>make download-ollama-model</code>. It will download the model present in the <code>OLLAMA_MODEL_NAME</code> var in the <code>.env</code> file.</li> <li>Run ollama to emulate openai : <code>make run-ollama</code> or ollamazure to emulate azure openai : <code>make run-ollamazure</code></li> <li>Run <code>make test-ollama</code>. You should see an output with a response.</li> </ul> </li> <li> <p>Or Cloud model: OpenAI or Azure OpenAI:</p> <ul> <li>Update the <code>.env</code> file (take a look at the <code>.env.example</code> file)</li> </ul> </li> <li> <p>Run <code>test-llm-client</code> to check if your the LLM responds.</p> </li> <li>To run the app with Streamlit (and without fastapi), run <code>make run-frontend</code></li> <li>To run the app with both Streamlit and FastAPI, run <code>make run-app</code></li> </ol>"},{"location":"#optional-evaluation-tool-ai-promptfoo-ragas","title":"(optional) Evaluation Tool (AI, Promptfoo, RAGAS)","text":"<ul> <li>To install the evaluation tool, run <code>make install-promptfoo</code> (nvm, promptfoo ...),</li> <li>We will use the name <code>LLMAAJ</code> to refer to LLM_AS_A_JUDGE, which is used to evaluate the AI system in Ragas metrics and other metrics</li> <li>Update the <code>.env</code> file (take a look at the <code>.env.example</code> file)</li> <li>To run the app, run <code>make eval-env-file</code></li> </ul> <p>How to run the evaluations locally:</p> <ul> <li>You can run promptfoo with <code>make eval-env-file</code> to run the evaluation reading the env file. take a look at promptfoo section in the <code>.env.example</code> file, if you want to use similarity metrics, you need to add the embedding model.</li> <li>Run <code>test-llmaaj-client</code> to check if the LLM as a judge responds.</li> <li>If you check the <code>make eval-env-file</code> command in the makefile, you can see that we do cd src and then set the python path to the current directory so promptfoo can find it.</li> <li>You need to be familiar with Promptfoo to use it. I advise you to learn about it before making changes to the evaluations part of this repository. For now, there are different datasets and configs:</li> <li>a simple dataset</li> <li>a dataset for information extraction (NER). IE metrics will be also computed on this dataset.</li> <li>if you want to change a config, go to the makefile and look at the <code>eval-env-file</code> command. Change the --config evaluation/promptfooconfig.yaml to other configs</li> </ul> <p>Steps to evaluate a rag system:</p> <p>retrieval metrics :evaluating retrieved contexts against ground truth:</p> <ol> <li>dataset :</li> <li>Each row of a dataset has a query and an answer.</li> <li>steps and metrics:</li> <li>Evaluate the chunking (chunk_size &amp; chunk_overlap): you will need to check the context_recall &amp; precision for example to see if the answer is in the context.</li> <li>Evaluate the embeddings: (try different models): same as previous</li> <li>try retrieval systems like Azure AI Search: same as previous</li> </ol> <p>end-task: evaluating ground truth vs generated answer:</p> <ol> <li>dataset :</li> <li>Depending on the end task and the type of the answer (string, or json...), you will need to build the dataset on that.</li> <li>For example, if the answer is a string, the dataset column 'answer' will be a string.</li> <li> <p>if you are going to evaluate batch of questions and batch of answers, you will need to build the dataset on that.</p> </li> <li> <p>steps and metrics:</p> </li> <li>For example, if the answer is a string, you will need to use the exact_match &amp; ragas answer_similarity.</li> <li>If the answer is a json, you will need to use the json_exact_match, missing_fields...</li> <li>Changing the prompt</li> <li> <p>Using one question, or multiple questions (batch) at once sent to the LLM.</p> </li> <li> <p>promptfoo config:</p> </li> <li>If you use a config like the one in <code>src/evaluation/providers/config_baseline.py</code>, we use an abstract class to define the global structure.</li> <li>Be aware that you need to adapt it if you want to use it for something else.</li> </ol>"},{"location":"#docker-installation-deprecated-will-be-updated-soon","title":"Docker installation (deprecated, will be updated soon)","text":"<ul> <li>Set the environment variables (in the system or in a .env file)</li> <li>Run docker with the right port bindings.</li> <li>Since the app is running in docker and using streamlit, the Internal and External URL addresses won't work. You need to access the app with localhost:forwarded_port</li> </ul>"},{"location":"#steps-for-installation-contributors-and-maintainers","title":"\u2699\ufe0f Steps for Installation (Contributors and maintainers)","text":"<p>Check the CONTRIBUTING.md file for more information.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Check the CONTRIBUTING.md file for more information.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at &lt;&gt;. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4 and 2.0, and was generated by contributing-gen.</p>"},{"location":"CONTRIBUTING/","title":"CONTRIBUTING","text":""},{"location":"CONTRIBUTING/#contributing-to-ai-project-template","title":"Contributing to ai-project-template","text":"<p>First off, thanks for taking the time to contribute! \u2764\ufe0f</p>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>This project and everyone participating in it is governed by the Code of Conduct. By participating, you are expected to uphold this code. Please report unacceptable behavior.</p>"},{"location":"CONTRIBUTING/#contributing","title":"Contributing","text":""},{"location":"CONTRIBUTING/#team-members","title":"Team members:","text":"<ul> <li>Amine Djeghri</li> </ul>"},{"location":"CONTRIBUTING/#steps-for-installation-contributors-and-maintainers","title":"\u2699\ufe0f Steps for Installation (Contributors and maintainers)","text":"<ul> <li>The first step is to install to read and test the project as a user</li> <li>To install the dev dependencies (pre-commit, pytest, ruff...), run <code>make install-dev</code></li> <li>run <code>make pre-commit install</code> to install pre-commit hooks</li> <li>To install the GitHub actions locally, run <code>make install-act</code></li> <li> <p>To install the gitlab ci locally, run <code>make install-ci</code></p> </li> <li> <p>Before you start working on an issue, please comment on (or create) the issue and wait for it to be assigned to you. If someone has already been assigned but didn't have the time to work on it lately, please communicate with them and ask if they're still working on it. This is to avoid multiple people working on the same issue. Once you have been assigned an issue, you can start working on it. When you are ready to submit your changes, open a pull request. For a detailed pull request tutorial, see this guide.</p> </li> <li> <p>Create a branch from the dev branch and respect the naming convention: <code>feature/your-feature-name</code>    or <code>bugfix/your-bug-name</code>.</p> </li> <li>Before commiting your code :</li> <li>Run <code>make test</code> to run the tests</li> <li>Run <code>make pre-commit</code> to check the code style &amp; linting. If it fails because gitguardien api key is not in your secret, add it in .secret. ggshield provides it for free.</li> <li>Run <code>make deploy-doc-local</code> to update the documentation</li> <li>(optional) Commit Messages: This project uses Gitmoji for commit messages. It helps to      understand the purpose of the commit through emojis. For example, a commit message with a bug fix can be prefixed with      \ud83d\udc1b. There are also Emojis in GitHub</li> <li>Manually, merge dev branch into your branch to solve and avoid any conflicts. Merging strategy: merge : dev \u2192      your_branch</li> <li>After merging, run <code>make test</code> and <code>make pre-commit</code> again to ensure that the tests are still passing.</li> <li>(if your project is a python package) Update the package\u2019s version, in pyproject.toml &amp; build the wheel</li> <li>Depending on the platform you use, run <code>make act</code> for GitHub Actions or <code>make gitlab-ci-local</code> for GitLab CI.</li> <li>Create a pull request. If the GitHub actions pass, the PR will be accepted and merged to dev.</li> </ul>"},{"location":"CONTRIBUTING/#for-repository-maintainers-merging-strategies-github-actions-guidelines","title":"(For repository maintainers) Merging strategies &amp; GitHub actions guidelines**","text":"<ul> <li>Once the dev branch is tested, the pipeline is green, and the PR has been accepted, you can merge with a 'merge'   strategy.</li> <li>DEV \u2192 MAIN: Then, you should create a merge from dev to main with Squash strategy.</li> <li>MAIN \u2192 RELEASE: The status of the ticket will change then to 'done.'</li> </ul>"},{"location":"package/main_backend/","title":"Main backend","text":""},{"location":"package/main_frontend/","title":"Main frontend","text":""},{"location":"package/settings/","title":"Settings","text":""},{"location":"package/settings/#src.settings.AzureAISearchEnvironmentVariables","title":"<code>AzureAISearchEnvironmentVariables</code>","text":"<p>               Bases: <code>BaseEnvironmentVariables</code></p> <p>Represents environment variables for configuring Azure AI Search and Azure Storage.</p> Source code in <code>src/settings.py</code> <pre><code>class AzureAISearchEnvironmentVariables(BaseEnvironmentVariables):\n    \"\"\"Represents environment variables for configuring Azure AI Search and Azure Storage.\"\"\"\n\n    ################ Azure Search settings ################\n    ENABLE_AZURE_SEARCH: bool = False\n    AZURE_SEARCH_SERVICE_ENDPOINT: Optional[str] = None\n    AZURE_SEARCH_INDEX_NAME: Optional[str] = None\n    AZURE_SEARCH_INDEXER_NAME: Optional[str] = None\n    AZURE_SEARCH_API_KEY: Optional[str] = None\n    AZURE_SEARCH_TOP_K: Optional[str] = \"2\"\n    SEMENTIC_CONFIGURATION_NAME: Optional[str] = None\n    # Azure Storage settings\n    AZURE_STORAGE_ACCOUNT_NAME: Optional[str] = None\n    AZURE_STORAGE_ACCOUNT_KEY: Optional[str] = None\n    AZURE_CONTAINER_NAME: Optional[str] = None\n\n    def get_azure_search_env_vars(self):\n        items_dict = {\n            \"ENABLE_AZURE_SEARCH\": self.ENABLE_AZURE_SEARCH,\n            \"SEMENTIC_CONFIGURATION_NAME\": self.SEMENTIC_CONFIGURATION_NAME,\n            \"AZURE_STORAGE_ACCOUNT_NAME\": self.AZURE_STORAGE_ACCOUNT_NAME,\n            \"AZURE_STORAGE_ACCOUNT_KEY\": self.AZURE_STORAGE_ACCOUNT_KEY,\n            \"AZURE_CONTAINER_NAME\": self.AZURE_CONTAINER_NAME,\n        }\n\n        items_dict.update(\n            {key: value for key, value in vars(self).items() if key.startswith(\"AZURE_SEARCH\")}\n        )\n        return items_dict\n\n    @model_validator(mode=\"after\")\n    def check_ai_search_keys(self: Self) -&gt; Self:\n        \"\"\"Validate API keys based on the selected provider after model initialization.\"\"\"\n        if self.ENABLE_AZURE_SEARCH:\n            azure_search_vars = self.get_azure_search_env_vars()\n            if any(value is None for value in azure_search_vars.values()):\n                loguru_logger.error(\n                    \"\\nAZURE_SEARCH environment variables must be provided when ENABLE_AZURE_SEARCH is True.\"\n                    f\"\\n{pretty_repr(azure_search_vars)}\"\n                )\n                raise ValueError(\n                    \"\\nAZURE_SEARCH environment variables must be provided when ENABLE_AZURE_SEARCH is True.\"\n                    f\"\\n{pretty_repr(azure_search_vars)}\"\n                )\n        return self\n</code></pre>"},{"location":"package/settings/#src.settings.AzureAISearchEnvironmentVariables.check_ai_search_keys","title":"<code>check_ai_search_keys()</code>","text":"<p>Validate API keys based on the selected provider after model initialization.</p> Source code in <code>src/settings.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_ai_search_keys(self: Self) -&gt; Self:\n    \"\"\"Validate API keys based on the selected provider after model initialization.\"\"\"\n    if self.ENABLE_AZURE_SEARCH:\n        azure_search_vars = self.get_azure_search_env_vars()\n        if any(value is None for value in azure_search_vars.values()):\n            loguru_logger.error(\n                \"\\nAZURE_SEARCH environment variables must be provided when ENABLE_AZURE_SEARCH is True.\"\n                f\"\\n{pretty_repr(azure_search_vars)}\"\n            )\n            raise ValueError(\n                \"\\nAZURE_SEARCH environment variables must be provided when ENABLE_AZURE_SEARCH is True.\"\n                f\"\\n{pretty_repr(azure_search_vars)}\"\n            )\n    return self\n</code></pre>"},{"location":"package/settings/#src.settings.ChatEnvironmentVariables","title":"<code>ChatEnvironmentVariables</code>","text":"<p>               Bases: <code>OpenAIChatEnvironmentVariables</code>, <code>AzureOpenAIChatEnvironmentVariables</code></p> <p>Represents environment variables for configuring the chatbot and promptfoo providers.</p> Source code in <code>src/settings.py</code> <pre><code>class ChatEnvironmentVariables(OpenAIChatEnvironmentVariables, AzureOpenAIChatEnvironmentVariables):\n    \"\"\"Represents environment variables for configuring the chatbot and promptfoo providers.\"\"\"\n\n    LLM_PROVIDER: ProviderEnum  # openai or azure_openai\n    # if you want to emulate azure_openai or openai using ollama or ollamazure\n    OLLAMA_MODEL_NAME: Optional[str] = None  # \"phi3:3.8b-mini-4k-instruct-q4_K_M\"\n    OLLAMA_EMBEDDING_MODEL_NAME: Optional[str] = None  # \"all-minilm:l6-v2\"\n\n    @model_validator(mode=\"after\")\n    def check_chat_api_keys(self: Self) -&gt; Self:\n        \"\"\"Validate API keys based on the selected provider after model initialization.\"\"\"\n        if self.LLM_PROVIDER == ProviderEnum.openai:\n            openai_vars = self.get_openai_env_vars()\n            openai_vars[\"LLM_PROVIDER\"] = self.LLM_PROVIDER\n            if any(value is None for value in openai_vars.values()):\n                loguru_logger.error(\n                    \"\\nOPENAI environment variables must be provided when LLM_PROVIDER is 'openai'.\"\n                    f\"\\n{pretty_repr(openai_vars)}\"\n                )\n                raise ValueError(\n                    \"\\nOPENAI environment variables must be provided when LLM_PROVIDER is 'openai'.\"\n                    f\"\\n{pretty_repr(openai_vars)}\"\n                )\n\n        elif self.LLM_PROVIDER == ProviderEnum.azure_openai:\n            azure_openai_vars = self.get_azure_openai_env_vars()\n            azure_openai_vars[\"LLM_PROVIDER\"] = self.LLM_PROVIDER\n            if any(value is None for value in azure_openai_vars.values()):\n                loguru_logger.error(\n                    \"\\nAZURE_OPENAI environment variables must be provided when LLM_PROVIDER is 'azure_openai'.\"\n                    f\"\\n{pretty_repr(azure_openai_vars)}\"\n                )\n                raise ValueError(\n                    \"\\nAZURE_OPENAI environment variables must be provided when LLM_PROVIDER is 'azure_openai'.\"\n                    f\"\\n{pretty_repr(azure_openai_vars)}\"\n                )\n\n        else:\n            raise ValueError(f\"Unknown LLM_PROVIDER: {self.LLM_PROVIDER}\")\n\n        return self\n</code></pre>"},{"location":"package/settings/#src.settings.ChatEnvironmentVariables.check_chat_api_keys","title":"<code>check_chat_api_keys()</code>","text":"<p>Validate API keys based on the selected provider after model initialization.</p> Source code in <code>src/settings.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_chat_api_keys(self: Self) -&gt; Self:\n    \"\"\"Validate API keys based on the selected provider after model initialization.\"\"\"\n    if self.LLM_PROVIDER == ProviderEnum.openai:\n        openai_vars = self.get_openai_env_vars()\n        openai_vars[\"LLM_PROVIDER\"] = self.LLM_PROVIDER\n        if any(value is None for value in openai_vars.values()):\n            loguru_logger.error(\n                \"\\nOPENAI environment variables must be provided when LLM_PROVIDER is 'openai'.\"\n                f\"\\n{pretty_repr(openai_vars)}\"\n            )\n            raise ValueError(\n                \"\\nOPENAI environment variables must be provided when LLM_PROVIDER is 'openai'.\"\n                f\"\\n{pretty_repr(openai_vars)}\"\n            )\n\n    elif self.LLM_PROVIDER == ProviderEnum.azure_openai:\n        azure_openai_vars = self.get_azure_openai_env_vars()\n        azure_openai_vars[\"LLM_PROVIDER\"] = self.LLM_PROVIDER\n        if any(value is None for value in azure_openai_vars.values()):\n            loguru_logger.error(\n                \"\\nAZURE_OPENAI environment variables must be provided when LLM_PROVIDER is 'azure_openai'.\"\n                f\"\\n{pretty_repr(azure_openai_vars)}\"\n            )\n            raise ValueError(\n                \"\\nAZURE_OPENAI environment variables must be provided when LLM_PROVIDER is 'azure_openai'.\"\n                f\"\\n{pretty_repr(azure_openai_vars)}\"\n            )\n\n    else:\n        raise ValueError(f\"Unknown LLM_PROVIDER: {self.LLM_PROVIDER}\")\n\n    return self\n</code></pre>"},{"location":"package/settings/#src.settings.EvaluationEnvironmentVariables","title":"<code>EvaluationEnvironmentVariables</code>","text":"<p>               Bases: <code>BaseEnvironmentVariables</code></p> <p>Represents environment variables for configuring Promptfoo and ragas.</p> <p>Ragas uses LLM as a judge for its metrics.</p> Source code in <code>src/settings.py</code> <pre><code>class EvaluationEnvironmentVariables(BaseEnvironmentVariables):\n    \"\"\"Represents environment variables for configuring Promptfoo and ragas.\n\n    Ragas uses LLM as a judge for its metrics.\n\n    \"\"\"\n\n    ENABLE_EVALUATION: bool = False\n    # LLM as a Judge for RAGAS metrics\n    LLMAAJ_PROVIDER: Optional[ProviderEnum] = None  # openai or azure_openai\n\n    # if judge provider is openai\n    LLMAAJ_OPENAI_DEPLOYMENT_NAME: Optional[str] = None\n    LLMAAJ_OPENAI_API_KEY: Optional[SecretStr] = None\n    LLMAAJ_OPENAI_BASE_URL: Optional[str] = None\n    LLMAAJ_OPENAI_EMBEDDING_DEPLOYMENT_NAME: Optional[str] = None\n\n    # if judge provider is azure_openai\n    LLMAAJ_AZURE_OPENAI_DEPLOYMENT_NAME: str = \"phi3:3.8b-mini-4k-instruct-q4_K_M\"\n    LLMAAJ_AZURE_OPENAI_API_KEY: SecretStr = \"1234\"\n    LLMAAJ_AZURE_OPENAI_BASE_URL: str = \"http://localhost:4041\"\n    LLMAAJ_AZURE_OPENAI_API_VERSION: str = \"2024-10-01-preview\"\n    LLMAAJ_AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME: Optional[str] = \"all-minilm:l6-v2\"\n\n    def get_eval_env_vars(self):\n        items_dict = {\n            \"ENABLE_EVALUATION\": self.ENABLE_EVALUATION,\n            \"LLMAAJ_PROVIDER\": self.LLMAAJ_PROVIDER,\n        }\n        if self.LLMAAJ_PROVIDER == ProviderEnum.openai:\n            items_dict.update(\n                {key: value for key, value in vars(self).items() if key.startswith(\"LLMAAJ_OPENAI\")}\n            )\n        elif self.LLMAAJ_PROVIDER == ProviderEnum.azure_openai:\n            items_dict.update(\n                {\n                    key: value\n                    for key, value in vars(self).items()\n                    if key.startswith(\"LLMAAJ_AZURE_OPENAI\")\n                }\n            )\n        else:\n            raise ValueError(f\"Unknown LLMAAJ_PROVIDER: {self.LLMAAJ_PROVIDER}\")\n        return items_dict\n\n    @model_validator(mode=\"after\")\n    def check_eval_api_keys(self: Self) -&gt; Self:\n        \"\"\"Validate API keys based on the selected provider after model initialization.\"\"\"\n        if self.ENABLE_EVALUATION:\n            if not self.LLMAAJ_PROVIDER in [ProviderEnum.openai, ProviderEnum.azure_openai]:\n                loguru_logger.error(\n                    f\"Unsupported env variable LLMAAJ_PROVIDER with value: {self.LLMAAJ_PROVIDER}\"\n                )\n                raise ValueError(\n                    f\"Unsupported variable LLMAAJ_PROVIDER with value: {self.LLMAAJ_PROVIDER}\"\n                )\n            eval_vars = self.get_eval_env_vars()\n            if any(value is None for value in eval_vars.values()):\n                loguru_logger.error(\n                    \"\\nEVALUATION environment variables must be provided when ENABLE_EVALUATION is True.\"\n                    f\"\\n{pretty_repr(eval_vars)}\"\n                )\n                raise ValueError(\n                    \"\\nEVALUATION environment variables must be provided when ENABLE_EVALUATION is True.\"\n                    f\"\\n{pretty_repr(eval_vars)}\"\n                )\n\n        return self\n</code></pre>"},{"location":"package/settings/#src.settings.EvaluationEnvironmentVariables.check_eval_api_keys","title":"<code>check_eval_api_keys()</code>","text":"<p>Validate API keys based on the selected provider after model initialization.</p> Source code in <code>src/settings.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_eval_api_keys(self: Self) -&gt; Self:\n    \"\"\"Validate API keys based on the selected provider after model initialization.\"\"\"\n    if self.ENABLE_EVALUATION:\n        if not self.LLMAAJ_PROVIDER in [ProviderEnum.openai, ProviderEnum.azure_openai]:\n            loguru_logger.error(\n                f\"Unsupported env variable LLMAAJ_PROVIDER with value: {self.LLMAAJ_PROVIDER}\"\n            )\n            raise ValueError(\n                f\"Unsupported variable LLMAAJ_PROVIDER with value: {self.LLMAAJ_PROVIDER}\"\n            )\n        eval_vars = self.get_eval_env_vars()\n        if any(value is None for value in eval_vars.values()):\n            loguru_logger.error(\n                \"\\nEVALUATION environment variables must be provided when ENABLE_EVALUATION is True.\"\n                f\"\\n{pretty_repr(eval_vars)}\"\n            )\n            raise ValueError(\n                \"\\nEVALUATION environment variables must be provided when ENABLE_EVALUATION is True.\"\n                f\"\\n{pretty_repr(eval_vars)}\"\n            )\n\n    return self\n</code></pre>"},{"location":"package/settings/#src.settings.Settings","title":"<code>Settings</code>","text":"<p>               Bases: <code>ChatEnvironmentVariables</code>, <code>AzureAISearchEnvironmentVariables</code>, <code>EvaluationEnvironmentVariables</code></p> <p>Settings class for the application.</p> <p>This class is automatically initialized with environment variables from the .env file. It inherits from the following classes and contains additional settings for streamlit and fastapi - ChatEnvironmentVariables - AzureAISearchEnvironmentVariables - EvaluationEnvironmentVariables</p> Source code in <code>src/settings.py</code> <pre><code>class Settings(\n    ChatEnvironmentVariables, AzureAISearchEnvironmentVariables, EvaluationEnvironmentVariables\n):\n    \"\"\"Settings class for the application.\n\n    This class is automatically initialized with environment variables from the .env file.\n    It inherits from the following classes and contains additional settings for streamlit and fastapi\n    - ChatEnvironmentVariables\n    - AzureAISearchEnvironmentVariables\n    - EvaluationEnvironmentVariables\n\n    \"\"\"\n\n    FASTAPI_HOST: str = \"localhost\"\n    FASTAPI_PORT: int = 8080\n    STREAMLIT_PORT: int = 8501\n    DEV_MODE: bool = True\n\n    def get_active_env_vars(self):\n        env_vars = {\n            \"DEV_MODE\": self.DEV_MODE,\n            \"FASTAPI_PORT\": self.FASTAPI_PORT,\n            \"STREAMLIT_PORT\": self.STREAMLIT_PORT,\n        }\n\n        if self.LLM_PROVIDER == ProviderEnum.openai:\n            env_vars.update(self.get_openai_env_vars())\n        elif self.LLM_PROVIDER == ProviderEnum.azure_openai:\n            env_vars.update(self.get_azure_openai_env_vars())\n        else:\n            raise ValueError(f\"Unknown LLM_PROVIDER: {self.LLM_PROVIDER}\")\n\n        if self.ENABLE_AZURE_SEARCH:\n            env_vars.update(self.get_azure_search_env_vars())\n\n        if self.ENABLE_EVALUATION:\n            env_vars.update(self.get_eval_env_vars())\n\n        return env_vars\n</code></pre>"},{"location":"package/utils/","title":"Utils","text":""},{"location":"package/utils/#src.utils.check_llm_as_a_judge_client","title":"<code>check_llm_as_a_judge_client()</code>","text":"<p>Check the LLM as a judge client by sending a message to the model.</p> <p>Uses Langchain OpenAIChat ou Langchain AzureChatas the LLM client</p> Source code in <code>src/utils.py</code> <pre><code>def check_llm_as_a_judge_client():\n    \"\"\"Check the LLM as a judge client by sending a message to the model.\n\n    Uses Langchain OpenAIChat ou Langchain AzureChatas the LLM client\n    \"\"\"\n    try:\n        llmaaj_chat_client, llmaaj_client_embedding = get_llm_as_a_judge_client()\n\n        messages = [\n            (\n                \"system\",\n                \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n            ),\n            (\"human\", \"I love programming.\"),\n        ]\n        ai_msg = llmaaj_chat_client.invoke(messages)\n        logger.info(\n            f\"\\nEvaluation environment variables are: \\n{pretty_repr(settings.get_eval_env_vars())}\\n\"\n            f\"\\nmodel response: {ai_msg.content}\"\n        )\n\n    except Exception as e:\n        logger.error(\n            f\"Error in check_llm_as_a_judge_client:\"\n            f\"\\nevaluation environment variables are: {pretty_repr(settings.get_eval_env_vars())}\"\n        )\n        raise e\n</code></pre>"},{"location":"package/utils/#src.utils.check_llm_client","title":"<code>check_llm_client()</code>","text":"<p>Check the LLM client by sending a message to the model.</p> <p>Uses OpenAI/AzureOpenAI as the LLM client</p> Source code in <code>src/utils.py</code> <pre><code>def check_llm_client():\n    \"\"\"Check the LLM client by sending a message to the model.\n\n    Uses OpenAI/AzureOpenAI as the LLM client\n    \"\"\"\n    client, model_name = get_llm_client()\n    try:\n        chat_completion = client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Hi\",\n                }\n            ],\n            model=model_name,\n        )\n        logger.info(chat_completion)\n\n        logger.info(\n            f\"\\nActive environment variables are: \\n{pretty_repr(settings.get_active_env_vars())}\\n\"\n            f\"\\nmodel response: {chat_completion.choices[0].message.content}\"\n        )\n    except Exception as e:\n        logger.error(\n            f\"Error in check_llm_client:\"\n            f\"\\nActive environment variables are: {pretty_repr(settings.get_active_env_vars())}\"\n        )\n        raise e\n</code></pre>"},{"location":"package/utils/#src.utils.get_llm_as_a_judge_client","title":"<code>get_llm_as_a_judge_client()</code>","text":"<p>Initializes and returns a LLM as a judge client based on the configured provider.</p> <p>Depending on the value of <code>settings.LLMAAJ_PROVIDER</code>, this function will initialize either an OpenAI or AzureOpenAI client and embedding client from Langchain OpenAI library. It loads the client with the appropriate settings and logs the model being used.</p> <p>If <code>settings.ENABLE_EVALUATION</code> is False, the function will return <code>(None, None)</code> and log a warning.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing - the initialized client and the embedding client (AzureOpenAI and AzureOpenAIEmbeddings or OpenAI and OpenAIEmbeddings) - or <code>(None, None)</code> if evaluation is disabled.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the configured LLM provider is unsupported.</p> Source code in <code>src/utils.py</code> <pre><code>def get_llm_as_a_judge_client():\n    \"\"\"Initializes and returns a LLM as a judge client based on the configured provider.\n\n    Depending on the value of `settings.LLMAAJ_PROVIDER`, this function will initialize\n    either an OpenAI or AzureOpenAI client and embedding client from Langchain OpenAI library. It loads the client with\n    the appropriate settings and logs the model being used.\n\n    If `settings.ENABLE_EVALUATION` is False, the function will return `(None, None)` and log a warning.\n\n    Returns:\n        tuple: A tuple containing\n            - the initialized client and the embedding client (AzureOpenAI and AzureOpenAIEmbeddings or OpenAI and\n            OpenAIEmbeddings)\n            - or `(None, None)` if evaluation is disabled.\n\n    Raises:\n        ValueError: If the configured LLM provider is unsupported.\n    \"\"\"\n    if settings.ENABLE_EVALUATION:\n        if settings.LLMAAJ_PROVIDER == ProviderEnum.azure_openai:\n            client = AzureChatOpenAI(\n                azure_endpoint=settings.LLMAAJ_AZURE_OPENAI_BASE_URL,\n                deployment_name=settings.LLMAAJ_AZURE_OPENAI_DEPLOYMENT_NAME,\n                openai_api_key=settings.LLMAAJ_AZURE_OPENAI_API_KEY,\n                openai_api_version=settings.LLMAAJ_AZURE_OPENAI_API_VERSION,\n            )\n            embeddings_client = AzureOpenAIEmbeddings(\n                azure_endpoint=settings.LLMAAJ_AZURE_OPENAI_BASE_URL,\n                model=settings.LLMAAJ_AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n                openai_api_key=settings.LLMAAJ_AZURE_OPENAI_API_KEY,\n                openai_api_version=settings.LLMAAJ_AZURE_OPENAI_API_VERSION,\n            )\n            loguru_logger.info(\n                f\"Loaded LLMAAJ AzureOpenAI client with model: {settings.LLMAAJ_OPENAI_DEPLOYMENT_NAME}\"\n            )\n        elif settings.LLMAAJ_PROVIDER == ProviderEnum.openai:\n            client = ChatOpenAI(\n                model_name=settings.LLMAAJ_OPENAI_DEPLOYMENT_NAME,\n                openai_api_base=settings.LLMAAJ_OPENAI_BASE_URL,\n                openai_api_key=settings.LLMAAJ_OPENAI_API_KEY,\n            )\n            embeddings_client = OpenAIEmbeddings(\n                model=settings.LLMAAJ_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n                openai_api_base=settings.LLMAAJ_OPENAI_BASE_URL,\n                openai_api_key=settings.LLMAAJ_OPENAI_API_KEY,\n            )\n            loguru_logger.info(\n                f\"Loaded LLMAAJ OpenAI client with model: {settings.LLMAAJ_OPENAI_DEPLOYMENT_NAME}\"\n            )\n\n        else:\n            raise ValueError(f\"Unsupported LLM provider: {settings.LLMAAJ_PROVIDER}\")\n\n        return (\n            client,\n            embeddings_client,\n        )\n    else:\n        loguru_logger.warning(\n            \"Evaluation is disabled. Set ENABLE_EVALUATION to True to activate it.\"\n        )\n        return None, None\n</code></pre>"},{"location":"package/utils/#src.utils.get_llm_client","title":"<code>get_llm_client()</code>","text":"<p>Initializes and returns a language model client based on the configured provider.</p> <p>Depending on the value of <code>settings.LLM_PROVIDER</code>, this function will initialize either an OpenAI or AzureOpenAI client from OpenAI library. It loads the client with the appropriate settings and logs the model being used.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[object, str]</code> <p>A tuple containing the initialized client and the model name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the configured LLM provider is unsupported.</p> Source code in <code>src/utils.py</code> <pre><code>def get_llm_client() -&gt; tuple[object, str]:\n    \"\"\"Initializes and returns a language model client based on the configured provider.\n\n    Depending on the value of `settings.LLM_PROVIDER`, this function will initialize\n    either an OpenAI or AzureOpenAI client from OpenAI library. It loads the client with the appropriate\n    settings and logs the model being used.\n\n    Returns:\n        tuple: A tuple containing the initialized client and the model name.\n\n    Raises:\n        ValueError: If the configured LLM provider is unsupported.\n    \"\"\"\n    if settings.LLM_PROVIDER == ProviderEnum.openai:\n        from openai import OpenAI\n\n        client = OpenAI(\n            base_url=settings.OPENAI_BASE_URL,\n            api_key=settings.OPENAI_API_KEY.get_secret_value(),\n        )\n        model_name = settings.OPENAI_DEPLOYMENT_NAME\n        loguru_logger.info(f\"Loaded OpenAI client with model: {model_name}\")\n\n    elif settings.LLM_PROVIDER == ProviderEnum.azure_openai:\n        from openai import AzureOpenAI\n\n        client = AzureOpenAI(\n            api_key=settings.AZURE_OPENAI_API_KEY.get_secret_value(),\n            api_version=settings.AZURE_OPENAI_API_VERSION,\n            azure_endpoint=settings.AZURE_OPENAI_BASE_URL,\n        )\n        model_name = settings.AZURE_OPENAI_DEPLOYMENT_NAME\n        loguru_logger.info(f\"Loaded AzureOpenAI client with model: {model_name}\")\n\n    else:\n        raise ValueError(f\"Unsupported LLM provider: {settings.LLM_PROVIDER}\")\n\n    return client, model_name\n</code></pre>"},{"location":"package/utils/#src.utils.initialize","title":"<code>initialize()</code>","text":"<p>Initialize the settings, logger, and search client.</p> <p>Reads the environment variables from the .env file defined in the Settings class.</p> <p>Returns:</p> Type Description <p>settings</p> <p>loguru_logger</p> <p>search_client</p> Source code in <code>src/utils.py</code> <pre><code>def initialize():\n    \"\"\"Initialize the settings, logger, and search client.\n\n    Reads the environment variables from the .env file defined in the Settings class.\n\n    Returns:\n        settings\n        loguru_logger\n        search_client\n    \"\"\"\n    settings = Settings()\n    loguru_logger.remove()\n\n    if settings.DEV_MODE:\n        loguru_logger.add(sys.stderr, level=\"TRACE\")\n    else:\n        loguru_logger.add(sys.stderr, level=\"INFO\")\n\n    search_client = None\n    if settings.ENABLE_AZURE_SEARCH:\n        search_client = SearchClient(\n            settings.AZURE_SEARCH_SERVICE_ENDPOINT,\n            settings.AZURE_SEARCH_INDEX_NAME,\n            AzureKeyCredential(settings.AZURE_SEARCH_API_KEY),\n        )\n\n    return settings, loguru_logger, search_client\n</code></pre>"},{"location":"package/api/api/","title":"Api","text":""},{"location":"package/api/api_route/","title":"Api route","text":""},{"location":"package/api/api_route/#src.api.api_route.TagEnum","title":"<code>TagEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>API tags.</p> Source code in <code>src/api/api_route.py</code> <pre><code>class TagEnum(str, Enum):\n    \"\"\"API tags.\"\"\"\n\n    general = \"general\"\n    tag_example = \"tag_example\"\n</code></pre>"},{"location":"package/api/log_config/","title":"Log config","text":""},{"location":"package/evaluation/context/","title":"Context","text":""},{"location":"package/evaluation/configs/config_baseline/","title":"Config baseline","text":""},{"location":"package/evaluation/configs/config_json/","title":"Config json","text":""},{"location":"package/evaluation/configs/config_json/#src.evaluation.configs.config_json.call_api","title":"<code>call_api(prompt, options, context)</code>","text":"<p>Function used by default by promptfoo. Check the config_json.yml.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt used in the configuration file (prompts section of config_json.yml).</p> required <code>options</code> required <code>context</code> <code>dict</code> <p>A dictionary containing the other_vars and context return by the previous function get_var</p> required Source code in <code>src/evaluation/configs/config_json.py</code> <pre><code>def call_api(prompt, options, context) -&gt; dict[str, str]:\n    \"\"\"Function used by default by promptfoo. Check the config_json.yml.\n\n    Args:\n        prompt (str): The prompt used in the configuration file (prompts section of config_json.yml).\n        options:\n        context (dict): A dictionary containing the other_vars and context return by the previous function get_var\n\n\n    \"\"\"\n    query = safe_eval(context[\"vars\"][\"query\"])\n    output = {list(query.keys())[0]: \"test\"}\n    result = {\n        \"output\": json.dumps(output, ensure_ascii=False),\n    }\n\n    return result\n</code></pre>"},{"location":"package/evaluation/configs/config_json/#src.evaluation.configs.config_json.get_var","title":"<code>get_var(var_name, prompt, other_vars)</code>","text":"<p>Function used by default by promptfoo call from the column 'context' of the dataset used in config_json.yml (test_json.csv).</p> <p>This function returns the context that will be used in the following call_api function The context can be. For example, the retrieved list of documents this is an example, and we will return the context that is defined in the csv file other_vars contains the vars of the csv file. prompt contains the prompt in the config_json.yml</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt used in the configuration file (prompts section of config_json.yml).</p> required <code>other_vars</code> <code>dict</code> <p>A dictionary containing variables from a CSV file.</p> required Source code in <code>src/evaluation/configs/config_json.py</code> <pre><code>def get_var(var_name, prompt, other_vars):\n    \"\"\"Function used by default by promptfoo call from the column 'context' of the dataset used in config_json.yml (test_json.csv).\n\n    This function returns the context that will be used in the following call_api function\n    The context can be. For example, the retrieved list of documents\n    this is an example, and we will return the context that is defined in the csv file\n    other_vars contains the vars of the csv file. prompt contains the prompt in the config_json.yml\n\n    Args:\n        prompt (str): The prompt used in the configuration file (prompts section of config_json.yml).\n        other_vars (dict): A dictionary containing variables from a CSV file.\n\n    \"\"\"\n    context = [\n        \"The USA Supreme Court ruling on abortion has sparked intense debates and discussions not only within the country but also around the world.\",\n        \"Many countries look to the United States as a leader in legal and social issues, so the decision could potentially influence the policies and attitudes towards abortion in other nations.\",\n        \"The ruling may impact international organizations and non-governmental groups that work on reproductive rights and women's health issues.\",\n    ]\n    return {\"output\": json.dumps(context, ensure_ascii=False)}\n</code></pre>"},{"location":"package/evaluation/metrics/data_types/","title":"Data types","text":""},{"location":"package/evaluation/metrics/utils/","title":"Utils","text":""},{"location":"package/evaluation/metrics/information_extraction/entity_level/","title":"Entity level","text":""},{"location":"package/evaluation/metrics/information_extraction/exact_match_json/","title":"Exact match json","text":""},{"location":"package/evaluation/metrics/information_extraction/exact_match_json/#src.evaluation.metrics.information_extraction.exact_match_json.get_assert","title":"<code>get_assert(output, context)</code>","text":"<p>Evaluates the precision at k.</p> Source code in <code>src/evaluation/metrics/information_extraction/exact_match_json.py</code> <pre><code>def get_assert(output: str, context):\n    \"\"\"Evaluates the precision at k.\"\"\"\n    threshold = 0.99\n    llm_answer, true_answer = convert_to_json(output, context, threshold)\n\n    try:\n        model_true_answer = create_dynamic_model(true_answer)\n        true_answer = model_true_answer(**true_answer)\n\n        llm_answer = model_true_answer(**llm_answer)\n\n        if llm_answer == true_answer:\n            score = 1.0\n            reason = f\"{score} &gt; {threshold} = {score &gt; threshold}\"\n        else:\n            dict_a = llm_answer.model_dump()\n            dict_b = true_answer.model_dump()\n            differences = [key for key in dict_b.keys() if dict_a.get(key) != dict_b.get(key)]\n\n            score = round(float(1 - (len(differences) / len(llm_answer.model_fields))), 2)\n\n            reason = f\"{score} &gt; {threshold} = {score &gt; threshold}. Number of differences: {len(differences)}. Differences: {differences}\"\n\n    except ValidationError as e:\n        total_fields = len(llm_answer.model_fields)\n        errors_count = len(e.errors())\n        score = round(float(1 - (errors_count / total_fields)), 2)\n        reason = str(e)\n\n    return {\n        \"pass\": score &gt; threshold,\n        \"score\": score,\n        \"reason\": reason,\n    }\n</code></pre>"},{"location":"package/evaluation/metrics/information_extraction/missing_fields/","title":"Missing fields","text":""},{"location":"package/evaluation/metrics/information_extraction/missing_fields/#src.evaluation.metrics.information_extraction.missing_fields.get_assert","title":"<code>get_assert(output, context)</code>","text":"<p>Evaluates the precision at k.</p> Source code in <code>src/evaluation/metrics/information_extraction/missing_fields.py</code> <pre><code>def get_assert(output: str, context) -&gt; GradingResult:\n    \"\"\"Evaluates the precision at k.\"\"\"\n    threshold = 0.99\n\n    llm_answer, true_answer = convert_to_json(output, context, threshold)\n\n    try:\n        model_true_answer = create_dynamic_model(true_answer)\n        # true_answer = model_true_answer(**true_answer)\n\n        llm_answer = model_true_answer(**llm_answer)\n        null_fields = [key for key, value in llm_answer.model_dump().items() if value is None]\n\n        score = round(float(1 - (len(null_fields) / len(llm_answer.model_fields))), 2)\n\n        reason = (\n            f\"{score} &gt; {threshold} = {score &gt; threshold}. Number of null fields: {len(null_fields)}. \"\n            f\"null_fields: {null_fields}\"\n        )\n    except ValidationError as e:\n        error = validation_error_message(e)\n        total_fields = len(llm_answer.model_fields)\n        errors_count = len(error.errors())\n        score = float(1 - (errors_count / total_fields))\n        reason = str(error)\n\n    return {\n        \"pass\": score &gt; threshold,\n        \"score\": score,\n        \"reason\": reason,\n    }\n</code></pre>"},{"location":"package/evaluation/metrics/information_extraction/similarity_json/","title":"Similarity json","text":""},{"location":"package/evaluation/metrics/information_extraction/similarity_json/#src.evaluation.metrics.information_extraction.similarity_json.compare_pydantic_objects","title":"<code>compare_pydantic_objects(obj1, obj2, differences=None)</code>","text":"<p>Compare two Pydantic objects using cosine similarity.</p> Source code in <code>src/evaluation/metrics/information_extraction/similarity_json.py</code> <pre><code>def compare_pydantic_objects(\n    obj1: BaseModel, obj2: BaseModel, differences: list = None\n) -&gt; dict[str, float]:\n    \"\"\"Compare two Pydantic objects using cosine similarity.\"\"\"\n    result = {}\n    total_similarity = 0\n    similarity = 0\n    if not differences:\n        differences = obj1.model_fields\n\n    for field in differences:\n        value1 = getattr(obj1, field)\n        value2 = getattr(obj2, field)\n        if value1 != value2:\n            if value1 and value2:\n                embedding1 = llmaaj_embedding_client.embed_query(text=str(value1))\n                embedding2 = llmaaj_embedding_client.embed_query(text=str(value2))\n                similarity = round(cosine_similarity(embedding1, embedding2), 2)\n            else:\n                similarity = 0\n        else:\n            similarity = 1\n\n        result[field] = similarity\n        total_similarity += similarity\n    return result, total_similarity\n</code></pre>"},{"location":"package/evaluation/metrics/information_extraction/similarity_json/#src.evaluation.metrics.information_extraction.similarity_json.cosine_similarity","title":"<code>cosine_similarity(a, b)</code>","text":"<p>Calculate cosine similarity between two vectors.</p> Source code in <code>src/evaluation/metrics/information_extraction/similarity_json.py</code> <pre><code>def cosine_similarity(a: np.ndarray, b: np.ndarray) -&gt; float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n</code></pre>"},{"location":"package/evaluation/metrics/information_extraction/similarity_json/#src.evaluation.metrics.information_extraction.similarity_json.get_assert","title":"<code>get_assert(output, context)</code>","text":"<p>Evaluates the precision at k.</p> Source code in <code>src/evaluation/metrics/information_extraction/similarity_json.py</code> <pre><code>def get_assert(output: str, context):\n    \"\"\"Evaluates the precision at k.\"\"\"\n    threshold = 0.99\n    llm_answer, true_answer = convert_to_json(output, context, threshold)\n\n    try:\n        model_true_answer = create_dynamic_model(true_answer)\n        true_answer = model_true_answer(**true_answer)\n\n        llm_answer = model_true_answer(**llm_answer)\n\n        if llm_answer == true_answer:\n            score = 1.0\n            reason = f\"{score} &gt; {threshold} = {score &gt; threshold}\"\n        else:\n            dict_a = llm_answer.model_dump()\n            dict_b = true_answer.model_dump()\n            differences = [key for key in dict_b.keys() if dict_a.get(key) != dict_b.get(key)]\n\n            num_similar_fields = len(llm_answer.model_fields) - len(differences)\n\n            result, similarity = compare_pydantic_objects(llm_answer, true_answer, differences)\n            score = round(\n                float((num_similar_fields + similarity) / len(llm_answer.model_fields)),\n                2,\n            )\n\n            reason = f\"{score} &gt; {threshold} = {score &gt; threshold}. Number of differences: {len(differences)}. Differences: {result}\"\n\n    except ValidationError as e:\n        total_fields = len(llm_answer.model_fields)\n        errors_count = len(e.errors())\n        score = round(float(1 - (errors_count / total_fields)), 2)\n        reason = str(e)\n\n    return {\n        \"pass\": score &gt; threshold,\n        \"score\": score,\n        \"reason\": reason,\n    }\n</code></pre>"},{"location":"package/evaluation/metrics/order_aware/reciprocal_rank/","title":"Reciprocal rank","text":""},{"location":"package/evaluation/metrics/order_aware/reciprocal_rank/#src.evaluation.metrics.order_aware.reciprocal_rank.get_assert","title":"<code>get_assert(output, context)</code>","text":"<p>Evaluates the precision at k.</p> Source code in <code>src/evaluation/metrics/order_aware/reciprocal_rank.py</code> <pre><code>def get_assert(output: str, context) -&gt; GradingResult:\n    \"\"\"Evaluates the precision at k.\"\"\"\n    retrieved_docs = safe_eval(context[\"vars\"][\"context\"])\n    relevant_docs = safe_eval(context[\"vars\"][\"relevant_context\"])\n\n    score = 0\n    # compute Reciprocal Rank\n    try:\n        score = round(1 / (relevant_docs.index(retrieved_docs[0]) + 1), 2)\n    except ValueError:\n        score = -1\n\n    # threshold = context[\"test\"][\"metadata\"][\"threshold_ragas_as\"]\n    threshold = 0\n\n    if math.isnan(score):\n        score = 0.0\n\n    return {\n        \"pass\": score &gt; threshold,\n        \"score\": score,\n        \"reason\": f\"{score} &gt; {threshold} = {score &gt; threshold}\",\n    }\n</code></pre>"},{"location":"package/evaluation/metrics/order_unaware/f1_at_k/","title":"F1 at k","text":""},{"location":"package/evaluation/metrics/order_unaware/f1_at_k/#src.evaluation.metrics.order_unaware.f1_at_k.get_assert","title":"<code>get_assert(output, context)</code>","text":"<p>Calculates F1@k.</p> Source code in <code>src/evaluation/metrics/order_unaware/f1_at_k.py</code> <pre><code>def get_assert(output: str, context) -&gt; GradingResult:\n    \"\"\"Calculates F1@k.\"\"\"\n    precision = precision_at_k.get_assert(context=context, output=output)[\"score\"]\n    recall = recall_at_k.get_assert(context=context, output=output)[\"score\"]\n\n    if precision + recall == 0:\n        score = 0.0\n    else:\n        score = round(float(2 * (precision * recall) / (precision + recall)), 2)\n\n    # threshold = context[\"test\"][\"metadata\"][\"threshold_ragas_as\"]\n    threshold = 0\n\n    if math.isnan(score):\n        score = 0.0\n\n    return {\n        \"pass\": score &gt; threshold,\n        \"score\": score,\n        \"reason\": f\"{score} &gt; {threshold} = {score &gt; threshold}\",\n    }\n</code></pre>"},{"location":"package/evaluation/metrics/order_unaware/precision_at_k/","title":"Precision at k","text":""},{"location":"package/evaluation/metrics/order_unaware/precision_at_k/#src.evaluation.metrics.order_unaware.precision_at_k.get_assert","title":"<code>get_assert(output, context)</code>","text":"<p>Evaluates the precision at k.</p> Source code in <code>src/evaluation/metrics/order_unaware/precision_at_k.py</code> <pre><code>def get_assert(output: str, context) -&gt; GradingResult:\n    \"\"\"Evaluates the precision at k.\"\"\"\n    retrieved_docs = safe_eval(context[\"vars\"][\"context\"])\n    relevant_docs = safe_eval(context[\"vars\"][\"relevant_context\"])\n    k = os.environ.get(\"K\", 3)\n    retrieved_docs_at_k = retrieved_docs[:k]\n    relevant_count = sum([1 for doc in retrieved_docs_at_k if doc in relevant_docs])\n    score = float(relevant_count / k)\n\n    # threshold = context[\"test\"][\"metadata\"][\"threshold_ragas_as\"]\n    threshold = 0\n\n    if math.isnan(score):\n        score = 0.0\n\n    return {\n        \"pass\": score &gt; threshold,\n        \"score\": score,\n        \"reason\": f\"{score} &gt; {threshold} = {score &gt; threshold}\",\n    }\n</code></pre>"},{"location":"package/evaluation/metrics/order_unaware/recall_at_k/","title":"Recall at k","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_answer_correctness/","title":"Ragas answer correctness","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_answer_relevancy/","title":"Ragas answer relevancy","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_answer_similarity/","title":"Ragas answer similarity","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_context_entity_recall/","title":"Ragas context entity recall","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_context_precision/","title":"Ragas context precision","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_context_recall/","title":"Ragas context recall","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_context_utilization/","title":"Ragas context utilization","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_faithfulness/","title":"Ragas faithfulness","text":""},{"location":"package/evaluation/metrics/ragas_metrics/ragas_harmfulness/","title":"Ragas harmfulness","text":""},{"location":"package/ml/ai/","title":"Ai","text":""},{"location":"package/ml/ai/#src.ml.ai.get_completions","title":"<code>get_completions(messages, stream=False, response_model=None, max_tokens=1000, temperature=0, top_p=1, seed=100, full_response=False, client=None)</code>","text":"<p>Returns a response from the azure openai model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> required <code>stream</code> <code>bool</code> <code>False</code> <code>response_model</code> <code>BaseModel</code> <code>None</code> <code>monitor</code> required <code>max_tokens</code> <code>int</code> <code>1000</code> <code>temperature</code> <code>int</code> <code>0</code> <code>top_p</code> <code>int</code> <code>1</code> <code>seed</code> <code>int</code> <code>100</code> <code>full_response</code> <code>bool</code> <code>False</code> <code>client</code> <code>None</code> <p>Returns:</p> Name Type Description <code>response</code> <code>str | BaseModel | None</code> <p>str | BaseModel | None :</p> Source code in <code>src/ml/ai.py</code> <pre><code>def get_completions(\n    messages: list,\n    stream: bool = False,\n    response_model: BaseModel = None,  # Use Instructor library\n    max_tokens: int = 1000,\n    temperature: int = 0,\n    top_p: int = 1,\n    seed: int = 100,\n    full_response: bool = False,\n    client=None,\n) -&gt; str | BaseModel | None:\n    \"\"\"Returns a response from the azure openai model.\n\n    Args:\n        messages:\n        stream:\n        response_model:\n        monitor:\n        max_tokens:\n        temperature:\n        top_p:\n        seed:\n        full_response:\n        client:\n\n    Returns:\n        response : str | BaseModel | None :\n    \"\"\"\n    if not client:\n        client = chat_client\n\n    input_dict = {\n        \"model\": chat_model_name,\n        \"messages\": messages,\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"seed\": seed,\n        \"stream\": stream,\n    }\n    if response_model:\n        # if you use local models instead of openai models, the response_model feature may not work\n        client = instructor.from_openai(chat_client, mode=instructor.Mode.JSON)\n        input_dict[\"response_model\"] = response_model\n\n    if stream:\n        raise NotImplementedError(\"Stream is not supported right now. Please set stream to False.\")\n\n    try:\n        response = client.chat.completions.create(**input_dict)\n    except Exception as e:\n        logger.exception(f\"Error in chat GPT: {e}\")\n        logger.error(\"chat GPT response: None\")\n        return None\n\n    if full_response or response_model:\n        return response\n    else:\n        return response.choices[0].message.content\n</code></pre>"},{"location":"package/ml/ai/#src.ml.ai.get_rag_response","title":"<code>get_rag_response(user_input)</code>","text":"<p>Return the response after running RAG.</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> required <code>settings</code> required <code>conversation_id</code> required <p>Returns:</p> Name Type Description <code>response</code> Source code in <code>src/ml/ai.py</code> <pre><code>def get_rag_response(user_input):\n    \"\"\"Return the response after running RAG.\n\n    Args:\n        user_input:\n        settings:\n        conversation_id:\n\n    Returns:\n        response:\n\n    \"\"\"\n    logger.info(f\"Running RAG\")\n\n    context = get_related_document_ai_search(user_input)\n    formatted_user_input = f\"question :{user_input}, \\n\\n contexte : \\n{context}.\"\n    logger.info(f\"RAG - final formatted prompt: {formatted_user_input}\")\n\n    response = get_completions(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Tu est un chatbot qui r\u00e9pond aux questions.\",\n            },\n            {\"role\": \"user\", \"content\": formatted_user_input},\n        ],\n    )\n    return response\n</code></pre>"},{"location":"package/ml/ai/#src.ml.ai.run_azure_ai_search_indexer","title":"<code>run_azure_ai_search_indexer()</code>","text":"<p>Run the azure ai search index.</p> <p>Returns:</p> Name Type Description <code>res</code> <p>response</p> Source code in <code>src/ml/ai.py</code> <pre><code>def run_azure_ai_search_indexer():\n    \"\"\"Run the azure ai search index.\n\n    Returns:\n            res: response\n    \"\"\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"api-key\": settings.AZURE_SEARCH_API_KEY,\n    }\n    params = {\"api-version\": \"2024-07-01\"}\n    url = f\"{settings.AZURE_SEARCH_SERVICE_ENDPOINT}/indexers('{settings.AZURE_SEARCH_INDEXER_NAME}')/run\"\n\n    res = requests.post(url=url, headers=headers, params=params)\n    logger.debug(f\"run_azure_ai_search_index response: {res.status_code}\")\n    return res\n</code></pre>"},{"location":"package/pages/0_azure_chat/","title":"0 azure chat","text":""},{"location":"package/pages/1_azure_rag/","title":"1 azure rag","text":""},{"location":"package/pages/2_fastapi_azure_rag/","title":"2 fastapi azure rag","text":""},{"location":"package/pages/3_promptfoo/","title":"3 promptfoo","text":""},{"location":"package/pages/4_giskard/","title":"4 giskard","text":""}]}